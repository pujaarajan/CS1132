{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "import base64\n",
    "import numpy as np\n",
    "import random\n",
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Tweet Sentiment Analysis\n",
    "\n",
    "We have a dump of tweets you may use here: https://s3.amazonaws.com/aiq-assets/tweets.txt\n",
    "\n",
    "In this step, you’ll conduct sentiment analysis to determine the relative positivity and negativity of the tweets in the file. There are many ways to classify tweets as positive or negative, and the exact methods are left up to you. You’ll need to justify your choices as well as explain any tradeoffs with the method chosen. You are welcome to use any libraries you’d like, as long as your code handles the training and classification. You are also welcome to use pre-classified training sets as a means to train your model. The output of your algorithm should be two files with two columns, one containing a sorted list of positive tweets and their scores with the most positive tweets appearing first (positive.txt), and the second containing a sorted list of negative tweets and their scores with the most negative tweets appearing first (negative.txt).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_numpy_obj_hook(dct):\n",
    "    \"\"\"Decodes a previously encoded numpy ndarray with proper shape and dtype.\n",
    "    :param dct: (dict) json encoded ndarray\n",
    "    :return: (ndarray) if input was an encoded ndarray\n",
    "    \"\"\"\n",
    "    if isinstance(dct, dict) and '__ndarray__' in dct:\n",
    "        data = base64.b64decode(dct['__ndarray__'])\n",
    "        return np.frombuffer(data, dct['dtype']).reshape(dct['shape'])\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('beer_1000.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Json to list\n",
    "beer_text_sentiment = []\n",
    "for beer in data:\n",
    "    for review in data[beer]:\n",
    "            beer_text_sentiment.append((beer,review['text'],review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data(beer_input):\n",
    "    random.shuffle(beer_text_sentiment)\n",
    "    training_number = int(len(beer_text_sentiment)*.2)\n",
    "    training_text = beer_text_sentiment[:training_number]\n",
    "    train_data_df = pd.DataFrame(training_text).convert_objects(convert_numeric=True)\n",
    "    train_data_df.columns = [\"Beer\", \"Text\", \"Sentiment\"]\n",
    "    num = train_data_df._get_numeric_data()\n",
    "    num[num <= 3] = 0\n",
    "    num[num > 3] = 1\n",
    "    return train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:5: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "train_data_df = training_data(beer_text_sentiment).drop('Beer', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poured from a 12 oz can to a pint glass.  A - ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A: Poured a nice golden color with almost no b...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Served room temp in a Trois Pistoles snifter. ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A: It is a great inky black with a brown thin ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, ABC must have changed their recipe on thi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Poured from a 12 oz can to a pint glass.  A - ...        1.0\n",
       "1  A: Poured a nice golden color with almost no b...        1.0\n",
       "2  Served room temp in a Trois Pistoles snifter. ...        1.0\n",
       "3  A: It is a great inky black with a brown thin ...        1.0\n",
       "4  Wow, ABC must have changed their recipe on thi...        1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tweets.txt') as f:\n",
    "    content = f.readlines()\n",
    "#remove whitespace characters like `\\n` at the end of each line\n",
    "tweetstxt = [x.strip() for x in content] \n",
    "test_data_df = pd.DataFrame(tweetstxt)\n",
    "test_data_df.columns = ['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "            stemmed.append(item)\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(train_data_df.Text.tolist() + test_data_df.Text.tolist())\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bag-of-words linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words_linear_classifier(training_data):\n",
    "    # remember that corpus_data_features_nd contains all of our \n",
    "    # original train and test data, so we need to exclude\n",
    "    # the unlabeled test entries\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(\n",
    "            corpus_data_features_nd[0:len(train_data_df)], \n",
    "            train_data_df.Sentiment,\n",
    "            train_size=0.80, \n",
    "            random_state=1234)\n",
    "    #Now we are ready to train our classifier.\n",
    "    log_model = LogisticRegression()\n",
    "    log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    #Now we use the classifier to label our evaluation set. \n",
    "    #We can use either predict for classes or predict_proba for probabilities.\n",
    "    y_pred = log_model.predict(X_test)\n",
    "    #Finally, we can re-train our model with all the training data and use it for sentiment \n",
    "    #classification with the original (unlabeled) test set.\n",
    "    # train classifier\n",
    "    log_model = LogisticRegression()\n",
    "    log_model = log_model.fit(X=corpus_data_features_nd[0:len(train_data_df)], y=train_data_df.Sentiment)\n",
    "    # get predictions\n",
    "    test_pred = log_model.predict(corpus_data_features_nd[len(train_data_df):])\n",
    "    # get probability of predictions\n",
    "    test_prob = log_model.predict_proba(corpus_data_features_nd[len(train_data_df):])\n",
    "    return test_pred, test_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred, test_prob = bag_of_words_linear_classifier(train_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample and Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sentiment  = dict.fromkeys(test_data_df.Text)\n",
    "for x in xrange(len(test_data_df.Text)):\n",
    "    tweet_sentiment[test_data_df.Text[x]] = test_pred[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_index  = dict.fromkeys(test_data_df.index)\n",
    "for x in xrange(len(test_data_df.index)):\n",
    "    tweet_index[test_data_df.index[x]] = test_pred[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_prob  = dict.fromkeys(test_data_df.Text)\n",
    "for x in xrange(len(test_data_df.Text)):\n",
    "    tweet_prob[test_data_df.Text[x]] = test_prob[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_tweets = []\n",
    "positive_tweets = []\n",
    "\n",
    "for key, value in tweet_prob.iteritems():\n",
    "    if value[0]<value[1]:\n",
    "        positive_tweets.append((key, value[0]))\n",
    "    else:\n",
    "        negative_tweets.append((key, value[1]))\n",
    "        \n",
    "sorted_neg = sorted(negative_tweets, reverse=True, key=lambda x: x[1])\n",
    "sorted_pos = sorted(positive_tweets, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "with open(\"negative_output.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sorted_neg)\n",
    "    \n",
    "with open(\"positive_output.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sorted_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Summarization of Methods and Results\n",
    "\n",
    "In this final step, summarize the results you found in Step 2 however you see appropriate (summary statistics, visualization, etc..). Include a description of assumptions made that could affect results and possibilities for improving the accuracy of your model.\n",
    "\n",
    "To submit your results, zip your code, tweet data, any additional training data, and result files into a single directory. Include a simple README that describes how to run your classifier so that the results you send us can be duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57377"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54913.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tweet_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3918"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57377-53459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of 57377 tweets, 53459.0 were positive and 3918 were negative. This means the training data was skewed or the classifier simply wasn't good. Let's check the training data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1807.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.Sentiment.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_df.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2318-1793.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training data was not that skewed compared to our final result. It had 1793 positive beer reviews and 525 negative reviews. However, it's possible the positive reviews were much longer in total and many more words altogether compared to the negative reviews. This may somewhat make sense because people often write longwinded rave reviews for items they love as opposed to simply stating an item was bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "1. Positive and negative beer reviews are similar and use similar words to positive and negative tweets - bad assumption?\n",
    "2. LR assumes the dependent variable to be binary.\n",
    "3. LR assumes the observations are independent.\n",
    "3. I sorted the list from most positive to least positive, similarly for negative, using the probability of the outcome, which isn't the best method to do this. I did this because I originally thought I was classifying just positive and negative, and this was a quick hacky fix method I came up with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "How would I fix this?\n",
    "\n",
    "1. Use the right training set.\n",
    "2. Compare results of logistic regression to results of a Naive Bayes Classifier and a Support Vector Machine. \n",
    "3. Use a different method of defining positive and negative words and tweets including weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
